{"posts":[{"title":"从源码构建 Jetson-Perf：一步步指南","text":"在 NVIDIA Jetson 平台上进行性能分析时，perf 工具是不可或缺的利器。然而，由于硬件架构和内核版本的特殊性，直接使用预编译版本可能无法完全满足需求。本文将从源码开始，详细介绍如何在 Jetson Orin 和 X86 平台上构建 perf 工具，并解决编译过程中可能遇到的常见问题。 一、准备工作：源码下载Jetson Orin 平台 下载 L4T 驱动包访问 NVIDIA 开发者下载中心，搜索 Jetson Linux Driver Package (L4T)，选择与设备匹配的版本（如 R35.x 系列）。[注：需根据 Jetson 型号选择对应的版本，参考下图中的版本号。] 获取 BSP 源码下载完成后，找到 public_sources.tbz2 文件，这是构建内核和工具链所需的基础源码包。 Orin Drive 平台 查看当前系统的核版本 1uname -r 下载内核源码访问 Linux 内核镜像站，根据当前系统内核版本下载对应的源码包。例如：若内核版本为 5.15.116-rt-tegra，则下载 linux-5.15.116.tar.gz。 X86 平台 查看当前系统的核版本 1uname -r 下载内核源码访问 Linux 内核镜像站，根据当前系统内核版本下载对应的源码包。例如：若内核版本为 5.10.0，则下载 linux-5.10.tar.gz。 二、源码编译步骤编译环境准备1sudo apt install make gcc flex bison pkg-config -y Jetson Orin 编译流程12345678910tar -xjvf public_sources.tbz2 &amp;&amp; \\cd Linux_for_Tegra/source/public &amp;&amp; \\tar -xjvf kernel_src.tbz2 &amp;&amp; \\cd kernel/kernel-5.10/tools/perf &amp;&amp; \\make -j$(nproc) &amp;&amp; \\sudo make install &amp;&amp; \\./perf --version Orin Drive 编译流程12345678tar zxvf linux-5.15.116.tar.gz &amp;&amp; \\cd linux-5.15.116/tools/perf &amp;&amp; \\make -j$(nproc) &amp;&amp; \\sudo make install &amp;&amp; \\./perf --version X86 平台编译流程 安装依赖项X86 平台需安装以下开发库： 1234sudo apt-get install build-essential flex bison python3 python3-dev \\ libelf-dev libnewt-dev libdw-dev libaudit-dev libiberty-dev \\ libunwind-dev libcap-dev libzstd-dev libnuma-dev libssl-dev \\ binutils-dev gcc-multilib liblzma-dev 编译 perf 1234567tar zxvf linux-5.10.tar.gz &amp;&amp; \\cd linux-5.10/tools/perf &amp;&amp; \\make -j$(nproc) &amp;&amp; \\sudo make install &amp;&amp; \\./perf --version 三、开启内核调试支持默认情况下，Linux 内核会限制 perf 的权限。通过以下命令解除限制： 1sudo sysctl kernel.perf_event_paranoid=-1 将此设置永久生效，可将其写入 /etc/sysctl.conf。 四、通过 perf 生成火焰图：直观分析性能瓶颈火焰图（Flame Graph）是性能分析的利器，它能以可视化方式展示程序的函数调用栈和资源占用情况。结合 perf 工具，我们可以快速生成火焰图，精准定位性能瓶颈。以下是详细操作步骤： 1. 安装火焰图生成工具首先需要克隆 Brendan Gregg 的 FlameGraph 工具库： 12git clone https://github.com/brendangregg/FlameGraph.gitcd FlameGraph 将此工具的路径加入环境变量，或直接通过绝对路径调用脚本（如 ./FlameGraph/stackcollapse-perf.pl）。 2. 使用 perf 采集性能数据运行目标程序，并通过 perf record 记录性能事件（如 CPU 周期、缓存失效等）： 123456# 监控 CPU 使用情况（默认）sudo perf record -F 99 -a -g -- sleep 60# 或监控特定进程sudo perf record -F 99 -p &lt;PID&gt; -g -- sleep 30# 或监控具体程序sudo perf record -F 99 -g ./&lt;MY_PROGRAM&gt; 参数说明： -F 99：采样频率为 99 Hz（避免与某些内核频率冲突）。 -a：监控所有 CPU。 -g：记录调用栈（生成火焰图必需）。 -- sleep 60：持续采样 60 秒。 3. 生成火焰图采集完成后，按以下步骤处理数据： 12345678# 生成 perf.data 的解析文本sudo perf script &gt; out.perf# 折叠调用栈（关键步骤）./FlameGraph/stackcollapse-perf.pl out.perf &gt; out.folded# 生成 SVG 格式火焰图./FlameGraph/flamegraph.pl out.folded &gt; flamegraph.svg 4. 查看与分析火焰图用浏览器打开 flamegraph.svg，可以看到类似下图的交互式可视化结果： 火焰图解读技巧： 横向宽度：表示函数或代码路径的资源（如 CPU 时间）占用比例。 纵向层级：表示函数调用栈的深度，顶层是正在执行的函数，下层是调用者。 点击缩放：支持点击任意区块展开查看细节。 高亮搜索：按 Ctrl + F 搜索关键函数名。 五、常见问题解决（FAQ）Q1：编译时提示某些 Feature 未启用问题现象编译过程中出现 WARNING: ... missing xxx,某些功能将被禁用。 解决方法 查看具体错误日志： 1cat ../build/feature/test-libunwind.make.output 根据缺失的依赖安装对应开发库。安装elf即可sudo apt install -y libelf-dev，其余Feature开启方法同理 123456sudo apt install -y libelf-devsudo apt install libunwind-devsudo apt install -y libdw1sudo apt install -y elfutilssudo apt install -y libdw-devsudo apt install -y binutils-dev Q2：运行 perf 时权限不足若未设置 kernel.perf_event_paranoid，需通过 sudo 运行 perf，或按第三步永久解除限制。 六、总结通过从源码构建 perf 工具，可以确保其与当前内核版本的完全兼容性，并启用所有高级功能。如果在编译中遇到问题，请优先检查依赖项和错误日志，大多数问题均可通过安装缺失的库解决。 效果验证成功编译后，运行 perf list 可查看支持的性能监控事件。尝试使用 perf stat 或 perf record 分析程序性能。","link":"/2025/02/17/%E4%BB%8E%E6%BA%90%E7%A0%81%E6%9E%84%E5%BB%BAJetson-Perf/"},{"title":"当CUDA撞上DLA暗礁：一个DLA死锁引发的蝴蝶效应与安全气囊的诞生","text":"在 Jetson AGX Orin/Orin-X 平台的实际运行中，CUDA 与 DLA 原本各司其职，却因并发而偶发集体“假死”——所有线程在 cudaStreamSynchronize() 阻塞中寸步难行。面对这一似乎无解的死锁暗礁，我从故障复现场景入手，定位到混合推理流中的同步点；随后排除了显存越界、流类型、DLA 降频等常见因素；最终灵感来源于“反向隔离”思路——为 GPU 与 DLA 各自配置独立的 GPU Context，并通过显式的异步拷贝与同步保证数据无缝切换。本文将以最直接的技术视角——从现象描述、排查方法到“多 Context 安全气囊”实现细节——全流程解读如何在 7×24h 压测中彻底消除死锁，实现并行推理的性能与可靠性双赢。 一、问题现象1. 偶发死锁 症状描述：单进程内，所有线程在调用 GPU 时集体“假死”——cudaStreamSynchronize() 一直阻塞，tegrastats 显示 GPU 占用率维持在某个固定值，不再波动。 复现难度： 极不稳定：有时运气极差，一周都无法触发；有时运气极好，仅需两小时即复现。 DLA 与 CUDA 混合推理时尤甚：DLA 推理线程先“提前几毫秒”挂死，其他线程随后也步入停滞。 GPU推理线程 与 DLA推理线程 同时卡死 2. 环境特征 硬件平台：Jetson AGX Orin 系列（最初使用 AGX Orin，后转至 Orin-X）。 推理流程： 全部模型均通过 TensorRT 的 enqueueV3() + CUDA Stream 异步执行； 前处理同样基于 CUDA Stream； 部分子图运行于 DLA，非支持算子回落至 Tensor/Tensor Core。 二、初步排查1. 定位死锁环节通过在各关键步骤间插入 cudaStreamSynchronize()，逐步缩小排查范围： 前处理 —— 无异常； TensorRT enqueueV3() —— 成功返回； 自定义 Plugin（HWCBGR → CHWRGB）—— 顺利完成； 同步点 —— 在 Plugin 之后的第一次 cudaStreamSynchronize() 挂死。 2. 排除思路 显存越界？ 其中一个模型 A 曾在 2023 年出现过显存越界，但通常显存越界会触发 sticky error 并抛出 illegal memory access 而非死锁。 Blocking vs. Non-blocking Stream？ 初步猜测 DLA 可能依赖于 blocking stream，而我使用的是 non-blocking 版；最终未能证实。 DLA 频率降频？ 若 DLA 调用低于 2 Hz，会自动释放并重申请资源，导致延迟飙升。 提升调用频率后虽解决了降频，但死锁依旧。 三、环境变更与新线索1. 从 AGX Orin 到 Orin-X 2023 年 3 月，怀疑 AGX Orin DLA 变频机制有问题，遂放弃尝试； 2024 年 3 月，转战 Orin-X，DLA 频率稳定——但在并行运行其他 GPU 模型 A 时，依旧触发死锁。 2. 关键“怀疑模型 A” 模型 A 曾导致显存越界，虽然在最小化测试环境下并未复现死锁，但在系统中只要与 DLA 并行，死锁概率骤增，然而其余模型并不会导致该问题。 单独跑 DLA、单独跑模型 A 均正常，仅并行时“相逢即死锁”。 四、“安全气囊”诞生：多 Context 隔离策略1. 灵感来源面试一位候选人提到的 GPU 多进程中间件： “通过截获多进程的 GPU 请求，在后端融合 context，实现时分复用，从而提升GPU利用率。” 虽然该方案与我单进程场景相悖，却激发了一个思路——反其道而行之：既然单 context 并行 DLA + GPU 会死锁，何不为它们分别提供独立 context？ 2. 实施要点 上下文划分 Orin 系列拥有 2 个 DLA 引擎 + 1 个 GPU，引擎间资源隔离天然独立； 为 DLA 和 GPU 各自创建独立 TensorRT Context。 内存与数据同步 注意：在 CUDA Context 之外，显存、Host 映射等资源也无法跨 context 共享； 切换 Context 时，显式 cudaMemcpyAsync() 与 cudaStreamSynchronize() 确保数据同步与完整性。 并行执行 DLA 上跑模型 H； GPU 上跑模型 A、B、C、D、E； 彼此互不干扰，独立抢占各自算力。 五、最终验证 7×24 h 连续压测：自 2024 年 4 月以来，未再出现任何死锁。 蝴蝶效应终成蝶：原本令人头痛的 DLA 隐晦死锁，反而催生出一套“安全气囊”式的多 context 并行机制——既排除了模型 A 的潜在越界风险，又保障了 DLA 与 GPU 资源的稳健协同。 六、结语当我们在深度学习推理的微观世界里拨开层层迷雾，总会发现，看似偶然的卡死背后，往往是资源管理与并发调度的微妙博弈。通过多 Context 隔离，为 DLA 与 GPU 架起了一道“安全栅栏”，既兼顾了性能，又守护了系统的可用性——这，或许就是自动驾驶推理优化的下一块基石。 —— 致正在与 DLA、TensorRT、CUDA “暗礁”博弈的你","link":"/2025/04/19/%E5%BD%93CUDA%E6%B5%81%E6%92%9E%E4%B8%8ADLA%E6%9A%97%E7%A4%81%EF%BC%9A%E4%B8%80%E4%B8%AADLA%E6%AD%BB%E9%94%81%E5%BC%95%E5%8F%91%E7%9A%84%E8%9D%B4%E8%9D%B6%E6%95%88%E5%BA%94%E4%B8%8E%E5%AE%89%E5%85%A8%E6%B0%94%E5%9B%8A%E7%9A%84%E8%AF%9E%E7%94%9F/"},{"title":"轻量化部署：Xavier设备低成本运行DeepSeek+QWQ大模型","text":"在边缘设备上部署大语言模型一直被认为是算力密集型的’禁区’，但当我用一块Jetson Xavier AGX成功运行了DeepSeek-R1 14B和QWQ-32B模型时，推理速度稳定在 8 tokens/s (DeepSeek-R1 14B) / 4 tokens/s (QWQ-32B) 以上！本文将分享从环境配置到量化优化的完整方案，证明边缘设备同样可以成为轻量化AI的舞台。 一、硬件配置与挑战分析： 设备选型：NVIDIA Jetson Xavier AGX 核心参数：512核Volta GPU + 8核Carmel CPU / 32GB RAM / 32GB eMMC 系统环境：JetPack 5.1.3 (Ubuntu 20.04) / CUDA 11.4 / TensorRT 8.5.2 功耗模式：默认10W切换至MAXN模式 为什么选择DeepSeek &amp; QWQ？ 维度 QWQ-32B DeepSeek-R1-14B 参数量 32B 14B 架构类型 Decoder-Only（密集型） MoE（混合专家，16选2） 上下文窗口 训练支持32K 原生支持16K Xavier显存占用 20~22GB 6~8GB 推理速度 4.3 tokens/s 8.7 tokens/s 优势 长文本理解能力突出工业领域知识深度优化动态稀疏激活降低计算负载 代码生成能力领先多轮对话响应快MoE架构灵活适配多任务 部署三大难关 显存墙：FP16模型加载即需14GB+显存 → Jetson统一内存 架构差异：ARM平台与x86服务器的库兼容性问题 → Docker容器化 计算瓶颈：Token生成速度&lt;5tokens/s时用户体验差 → 流水线优化 二、技术实现全流程拆解阶段一：构建ARM适配的基础环境核心挑战：NVIDIA Jetson Xavier的ARM架构与常规x86服务器存在三大差异： 指令集差异：ARMv8.2与x86_64的SIMD指令集不兼容（如NEON vs AVX512） 内存管理差异：Jetson采用CPU-GPU统一内存架构（UMA），与传统PCIe分体式显存管理模式冲突 软件生态差异：PyTorch等框架的ARM预编译包缺失关键算子支持 技术路线：采用「容器化隔离+定制化编译」双轨策略： 硬件抽象层：通过 jetson-containers 项目预构建CUDA、cuDNN等核心组件的ARM64版本容器镜像 依赖解耦：针对JetPack 5.1.3的特定版本（CUDA 11.4 + TensorRT 8.5.2），使用镜像解决动态库符号链接问题 关键实现： 更新python版本：jetson-containers代码依赖Python 3.9（functools.cache），需要将原有python3.8更新为python3.9版本 12sudo rm /usr/bin/python3 &amp;&amp; \\sudo ln -s /usr/bin/python3.9 /usr/bin/python3 安装jetson-containers： 12git clone https://github.com/dusty-nv/jetson-containersbash jetson-containers/install.sh 设置Docker Runtime： 修改/etc/docker/daemon.json配置，并指定default-runtime 12345678910{ &quot;runtimes&quot;: { &quot;nvidia&quot;: { &quot;path&quot;: &quot;nvidia-container-runtime&quot;, &quot;runtimeArgs&quot;: [] } }, &quot;default-runtime&quot;: &quot;nvidia&quot;} 重启docker服务 1sudo systemctl restart docker 验证配置更新 1sudo docker info | grep 'Default Runtime' 显示docker没有root权限： 1sudo usermod -aG docker $USER 阶段二：构建Ollama基础镜像 修改Dockerfile 修改jetson-containers/packages/llm/ollama/Dockerfile为以下内容 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#---# name: ollama# group: llm# config: config.py# depends: cuda# requires: '&gt;=34.1.0'# docs: docs.md#---ARG BASE_IMAGE \\ CMAKE_CUDA_ARCHITECTURES \\ CUDA_VERSION_MAJOR \\ JETPACK_VERSION \\ OLLAMA_REPO \\ OLLAMA_BRANCH \\ GOLANG_VERSION \\ CMAKE_VERSION# build the runtime containerFROM ${BASE_IMAGE}ARG JETPACK_VERSION \\ CUDA_VERSION_MAJOREXPOSE 11434ENV OLLAMA_HOST=0.0.0.0 \\ OLLAMA_MODELS=/data/models/ollama/models \\ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\ LD_LIBRARY_PATH=/usr/local/cuda/lib:/usr/local/cuda/lib64:/usr/local/cuda/include:${LD_LIBRARY_PATH} \\ JETSON_JETPACK=${JETPACK_VERSION} \\ CUDA_VERSION_MAJOR=${CUDA_VERSION_MAJOR}RUN apt-get update &amp;&amp; \\ apt install -y curl &amp;&amp; \\ rm -rf /var/lib/apt/lists/* &amp;&amp; \\ apt-get cleanCOPY nv_tegra_release /etc/nv_tegra_release#ADD https://api.github.com/repos/ollama/ollama/branches/main /tmp/ollama_version.jsonRUN curl -H &quot;Authorization: token ${GITHUB_TOKEN}&quot; \\ -o /tmp/ollama_version.json \\ https://api.github.com/repos/ollama/ollama/branches/mainRUN curl -fsSL https://ollama.com/install.sh | shRUN rm /usr/bin/python || true &amp;&amp; \\ ln -s /usr/bin/python3 /usr/bin/pythonCOPY start_ollama /CMD /start_ollama &amp;&amp; /bin/bash 构建镜像 1jetson-containers build ollama 验证构建镜像 1docker images | grep ollama 阶段三：模型下载 &amp; 推理 进入Container环境 1jetson-containers run --name ollama $(autotag ollama) 模型下载 12ollama pull qwq:latestollama pull deepseek-r1:14b 模型推理验证 1ollama run qwq:latest 查看GPU使用率 1jtop jtop安装： 12sudo pip3 install -U jetson-stats &amp;&amp; \\sudo systemctl restart jtop.service 阶段四（可选）：K3S部署与服务搭建待补充 三、性能实测：数据不说谎模型对比实验 模型 显存 待机功耗 运行功耗 prompt eval rate eval rate deepseek-r1:14b 10.5G 5.1w 43.7 w 144.21 tokens/s 8.11 tokens/s QwQ:32b 21.1G 5.4 w 47.6 w 6.04 tokens/s 3.93 tokens/s deepseek-r1:14b QwQ:32b 成本效益对比 方案 硬件成本 每月费用 延迟 数据出境风险 云端API ¥ 0 ¥1600 200ms 高 Xavier本地 ¥ 3000 ¥18 20ms 零 应用场景：这不是玩具，是生产力案例1：openweb-ui知识库本地问答待补充","link":"/2025/03/08/%E8%BD%BB%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2%EF%BC%9AXavier%E8%AE%BE%E5%A4%87%E4%BD%8E%E6%88%90%E6%9C%AC%E8%BF%90%E8%A1%8CDeepSeek-QWQ%E5%A4%A7%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"Orin","slug":"Orin","link":"/tags/Orin/"},{"name":"GPU","slug":"GPU","link":"/tags/GPU/"},{"name":"性能分析","slug":"性能分析","link":"/tags/%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"},{"name":"Xavier","slug":"Xavier","link":"/tags/Xavier/"}],"categories":[{"name":"blog","slug":"blog","link":"/categories/blog/"}],"pages":[{"title":"Silvester Hsu","text":"Who am IMy name is Silvester Hsu, and I specialize in model inference and GPU optimization, with a focus on autonomous driving technologies. Currently, My primary research interests include optimizing the performance of models for autonomous driving applications, improving the computational efficiency of deep learning models and fine-tuning system-level GPU performance to meet the high demands of real-time, safety-critical applications. I graduated with a Master’s degree in Computer Science from University of Sheffield where I conducted research in machine learning and Nvidia optimization techniques. In my work, I strive to balance theoretical elegance with real-world applicability. I am passionate about creating scalable, efficient systems that can make a tangible difference. However, I am also acutely aware of the trade-offs between theory and implementation, and often reflect on how optimization can help overcome some of these challenges. Career JourneyI was initially drawn to technology due to its ability to solve real-world problems at scale. However, my early experiences with traditional computer science techniques made me question the existing paradigms. My curiosity led me to machine learning and AI, fields I now consider both challenging and immensely rewarding. While pursuing my Master’s, I started engaging with open-source projects and attending GPU optimization in machine learning, which allowed me to quickly catch up with the latest advancements in AI. These experiences, along with collaborations with researchers and practitioners, have shaped my expertise in building both state-of-the-art models and practical engineering solutions. About This SiteThis website was created to document my learning journey, share insights from my work in machine learning, and present some of my projects and research. It’s a place for me to explore new ideas, dive into complex technical topics, and contribute to the broader AI community. The main objectives of this site are: Discuss the latest advancements in GPU optimization for machine learning, with a special focus on autonomous driving. Share practical strategies for improving computational efficiency in deep learning. Present personal projects, research, and experiments. Reflect on key milestones and achievements in my career. Please note, all views expressed on this site are my own and do not represent those of my current or past employers. CitationsIf you would like to cite any of the content on this site or my open-source projects on GitHub, please refer to the URLs provided. I will ensure they remain permanent, unless GitHub or my hosting provider makes significant changes in the future. Contact MeIf you have any questions, comments, or feedback about my blog posts or projects, feel free to leave a comment under the respective post or open an issue in my GitHub repositories. I highly encourage discussions that can help others. For more private inquiries, you can reach me via email.","link":"/index.html"},{"title":"Silvester Hsu","text":"I am a developer dedicated to efficient algorithms and GPU programming, with a focus on research and application in computer vision, deep learning, and high-performance computing. I have extensive experience in parallel computing, image processing, and inference engine optimization. I specialize in performance optimization using CUDA and have a deep understanding of GPU memory management and acceleration of deep learning frameworks. In my GitHub repository, you can find open-source projects across multiple domains, ranging from GPU memory management tools and image distortion correction algorithms to efficient model inference engine optimizations. I have contributed code to several deep learning projects and successfully improved inference speed and computational efficiency. Tech Stack Programming Languages: C++, Python, Bash GPU Programming: CUDA, TensorRT, cuDNN Deep Learning Frameworks: TensorFlow, PyTorch, ONNX Image Processing: OpenCV, Computer Vision (CV) Databases: SQLite Operating Systems: Linux (Ubuntu), Windows Tools &amp; Environments: Git, Docker, Jenkins, CMake Performance Optimization: SIMD, Multithreading, High-Performance Computing (HPC) I have a strong interest in algorithm optimization, computational graph design, and GPU acceleration, particularly in applications such as autonomous driving and image recognition. I look forward to connecting and collaborating with like-minded developers to push the boundaries of cutting-edge technology.","link":"/about/index.html"}]}