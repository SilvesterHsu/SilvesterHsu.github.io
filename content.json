{"posts":[{"title":"从源码构建 Jetson-Perf：一步步指南","text":"在 NVIDIA Jetson 平台上进行性能分析时，perf 工具是不可或缺的利器。然而，由于硬件架构和内核版本的特殊性，直接使用预编译版本可能无法完全满足需求。本文将从源码开始，详细介绍如何在 Jetson Orin 和 X86 平台上构建 perf 工具，并解决编译过程中可能遇到的常见问题。 一、准备工作：源码下载Jetson Orin 平台 下载 L4T 驱动包访问 NVIDIA 开发者下载中心，搜索 Jetson Linux Driver Package (L4T)，选择与设备匹配的版本（如 R35.x 系列）。[注：需根据 Jetson 型号选择对应的版本，参考下图中的版本号。] 获取 BSP 源码下载完成后，找到 public_sources.tbz2 文件，这是构建内核和工具链所需的基础源码包。 Orin Drive 平台 查看当前系统的核版本 1uname -r 下载内核源码访问 Linux 内核镜像站，根据当前系统内核版本下载对应的源码包。例如：若内核版本为 5.15.116-rt-tegra，则下载 linux-5.15.116.tar.xz。 X86 平台 查看当前系统的核版本 1uname -r 下载内核源码访问 Linux 内核镜像站，根据当前系统内核版本下载对应的源码包。例如：若内核版本为 5.10.0，则下载 linux-5.10.tar.xz。 二、源码编译步骤编译环境准备1sudo apt install make gcc flex bison pkg-config -y Jetson Orin 编译流程12345678910tar -xjvf public_sources.tbz2 &amp;&amp; \\cd Linux_for_Tegra/source/public &amp;&amp; \\tar -xjvf kernel_src.tbz2 &amp;&amp; \\cd kernel/kernel-5.10/tools/perf &amp;&amp; \\make -j$(nproc) &amp;&amp; \\sudo make install &amp;&amp; \\./perf --version Orin Drive 编译流程12345678tar zxvf linux-5.15.116.tar.gz &amp;&amp; \\cd linux-5.15.116/tools/perf &amp;&amp; \\make -j$(nproc) &amp;&amp; \\sudo make install &amp;&amp; \\./perf --version X86 平台编译流程 安装依赖项X86 平台需安装以下开发库： 1234sudo apt-get install build-essential flex bison python3 python3-dev \\ libelf-dev libnewt-dev libdw-dev libaudit-dev libiberty-dev \\ libunwind-dev libcap-dev libzstd-dev libnuma-dev libssl-dev \\ binutils-dev gcc-multilib liblzma-dev 编译 perf 123456cd linux-5.10/tools/perfmake -j$(nproc)sudo make install./perf --version 三、开启内核调试支持默认情况下，Linux 内核会限制 perf 的权限。通过以下命令解除限制： 1sudo sysctl kernel.perf_event_paranoid=-1 将此设置永久生效，可将其写入 /etc/sysctl.conf。 四、通过 perf 生成火焰图：直观分析性能瓶颈火焰图（Flame Graph）是性能分析的利器，它能以可视化方式展示程序的函数调用栈和资源占用情况。结合 perf 工具，我们可以快速生成火焰图，精准定位性能瓶颈。以下是详细操作步骤： 1. 安装火焰图生成工具首先需要克隆 Brendan Gregg 的 FlameGraph 工具库： 12git clone https://github.com/brendangregg/FlameGraph.gitcd FlameGraph 将此工具的路径加入环境变量，或直接通过绝对路径调用脚本（如 ./FlameGraph/stackcollapse-perf.pl）。 2. 使用 perf 采集性能数据运行目标程序，并通过 perf record 记录性能事件（如 CPU 周期、缓存失效等）： 123456# 监控 CPU 使用情况（默认）sudo perf record -F 99 -a -g -- sleep 60# 或监控特定进程sudo perf record -F 99 -p &lt;PID&gt; -g -- sleep 30# 或监控具体程序sudo perf record -F 99 -g ./&lt;MY_PROGRAM&gt; 参数说明： -F 99：采样频率为 99 Hz（避免与某些内核频率冲突）。 -a：监控所有 CPU。 -g：记录调用栈（生成火焰图必需）。 -- sleep 60：持续采样 60 秒。 3. 生成火焰图采集完成后，按以下步骤处理数据： 12345678# 生成 perf.data 的解析文本sudo perf script &gt; out.perf# 折叠调用栈（关键步骤）./FlameGraph/stackcollapse-perf.pl out.perf &gt; out.folded# 生成 SVG 格式火焰图./FlameGraph/flamegraph.pl out.folded &gt; flamegraph.svg 4. 查看与分析火焰图用浏览器打开 flamegraph.svg，可以看到类似下图的交互式可视化结果： 火焰图解读技巧： 横向宽度：表示函数或代码路径的资源（如 CPU 时间）占用比例。 纵向层级：表示函数调用栈的深度，顶层是正在执行的函数，下层是调用者。 点击缩放：支持点击任意区块展开查看细节。 高亮搜索：按 Ctrl + F 搜索关键函数名。 五、常见问题解决（FAQ）Q1：编译时提示某些 Feature 未启用问题现象编译过程中出现 WARNING: ... missing xxx,某些功能将被禁用。 解决方法 查看具体错误日志： 1cat ../build/feature/test-libunwind.make.output 根据缺失的依赖安装对应开发库。安装elf即可sudo apt install -y libelf-dev，其余Feature开启方法同理 123456sudo apt install -y libelf-devsudo apt install libunwind-devsudo apt install -y libdw1sudo apt install -y elfutilssudo apt install -y libdw-devsudo apt install -y binutils-dev Q2：运行 perf 时权限不足若未设置 kernel.perf_event_paranoid，需通过 sudo 运行 perf，或按第三步永久解除限制。 六、总结通过从源码构建 perf 工具，可以确保其与当前内核版本的完全兼容性，并启用所有高级功能。如果在编译中遇到问题，请优先检查依赖项和错误日志，大多数问题均可通过安装缺失的库解决。 效果验证成功编译后，运行 perf list 可查看支持的性能监控事件。尝试使用 perf stat 或 perf record 分析程序性能。","link":"/2025/02/17/%E4%BB%8E%E6%BA%90%E7%A0%81%E6%9E%84%E5%BB%BAJetson-Perf/"}],"tags":[{"name":"Orin","slug":"Orin","link":"/tags/Orin/"},{"name":"性能分析","slug":"性能分析","link":"/tags/%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"}],"categories":[{"name":"blog","slug":"blog","link":"/categories/blog/"}],"pages":[{"title":"Silvester Hsu","text":"Who am IMy name is Silvester Hsu, and I specialize in model inference and GPU optimization, with a focus on autonomous driving technologies. Currently, My primary research interests include optimizing the performance of models for autonomous driving applications, improving the computational efficiency of deep learning models and fine-tuning system-level GPU performance to meet the high demands of real-time, safety-critical applications. I graduated with a Master’s degree in Computer Science from University of Sheffield where I conducted research in machine learning and Nvidia optimization techniques. In my work, I strive to balance theoretical elegance with real-world applicability. I am passionate about creating scalable, efficient systems that can make a tangible difference. However, I am also acutely aware of the trade-offs between theory and implementation, and often reflect on how optimization can help overcome some of these challenges. Career JourneyI was initially drawn to technology due to its ability to solve real-world problems at scale. However, my early experiences with traditional computer science techniques made me question the existing paradigms. My curiosity led me to machine learning and AI, fields I now consider both challenging and immensely rewarding. While pursuing my Master’s, I started engaging with open-source projects and attending GPU optimization in machine learning, which allowed me to quickly catch up with the latest advancements in AI. These experiences, along with collaborations with researchers and practitioners, have shaped my expertise in building both state-of-the-art models and practical engineering solutions. About This SiteThis website was created to document my learning journey, share insights from my work in machine learning, and present some of my projects and research. It’s a place for me to explore new ideas, dive into complex technical topics, and contribute to the broader AI community. The main objectives of this site are: Discuss the latest advancements in GPU optimization for machine learning, with a special focus on autonomous driving. Share practical strategies for improving computational efficiency in deep learning. Present personal projects, research, and experiments. Reflect on key milestones and achievements in my career. Please note, all views expressed on this site are my own and do not represent those of my current or past employers. CitationsIf you would like to cite any of the content on this site or my open-source projects on GitHub, please refer to the URLs provided. I will ensure they remain permanent, unless GitHub or my hosting provider makes significant changes in the future. Contact MeIf you have any questions, comments, or feedback about my blog posts or projects, feel free to leave a comment under the respective post or open an issue in my GitHub repositories. I highly encourage discussions that can help others. For more private inquiries, you can reach me via email.","link":"/index.html"},{"title":"Silvester Hsu","text":"I am a developer dedicated to efficient algorithms and GPU programming, with a focus on research and application in computer vision, deep learning, and high-performance computing. I have extensive experience in parallel computing, image processing, and inference engine optimization. I specialize in performance optimization using CUDA and have a deep understanding of GPU memory management and acceleration of deep learning frameworks. In my GitHub repository, you can find open-source projects across multiple domains, ranging from GPU memory management tools and image distortion correction algorithms to efficient model inference engine optimizations. I have contributed code to several deep learning projects and successfully improved inference speed and computational efficiency. Tech Stack Programming Languages: C++, Python, Bash GPU Programming: CUDA, TensorRT, cuDNN Deep Learning Frameworks: TensorFlow, PyTorch, ONNX Image Processing: OpenCV, Computer Vision (CV) Databases: SQLite Operating Systems: Linux (Ubuntu), Windows Tools &amp; Environments: Git, Docker, Jenkins, CMake Performance Optimization: SIMD, Multithreading, High-Performance Computing (HPC) I have a strong interest in algorithm optimization, computational graph design, and GPU acceleration, particularly in applications such as autonomous driving and image recognition. I look forward to connecting and collaborating with like-minded developers to push the boundaries of cutting-edge technology.","link":"/about/index.html"}]}