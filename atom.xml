<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SilvesterHsu</title>
  
  
  <link href="https://silvesterhsu.github.io/atom.xml" rel="self"/>
  
  <link href="https://silvesterhsu.github.io/"/>
  <updated>2025-05-16T06:30:46.786Z</updated>
  <id>https://silvesterhsu.github.io/</id>
  
  <author>
    <name>SilvesterHsu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>当CUDA撞上DLA暗礁：一个DLA死锁引发的蝴蝶效应与安全气囊的诞生</title>
    <link href="https://silvesterhsu.github.io/2025/04/19/%E5%BD%93CUDA%E6%B5%81%E6%92%9E%E4%B8%8ADLA%E6%9A%97%E7%A4%81%EF%BC%9A%E4%B8%80%E4%B8%AADLA%E6%AD%BB%E9%94%81%E5%BC%95%E5%8F%91%E7%9A%84%E8%9D%B4%E8%9D%B6%E6%95%88%E5%BA%94%E4%B8%8E%E5%AE%89%E5%85%A8%E6%B0%94%E5%9B%8A%E7%9A%84%E8%AF%9E%E7%94%9F/"/>
    <id>https://silvesterhsu.github.io/2025/04/19/%E5%BD%93CUDA%E6%B5%81%E6%92%9E%E4%B8%8ADLA%E6%9A%97%E7%A4%81%EF%BC%9A%E4%B8%80%E4%B8%AADLA%E6%AD%BB%E9%94%81%E5%BC%95%E5%8F%91%E7%9A%84%E8%9D%B4%E8%9D%B6%E6%95%88%E5%BA%94%E4%B8%8E%E5%AE%89%E5%85%A8%E6%B0%94%E5%9B%8A%E7%9A%84%E8%AF%9E%E7%94%9F/</id>
    <published>2025-04-19T10:33:42.000Z</published>
    <updated>2025-05-16T06:30:46.786Z</updated>
    
    <content type="html"><![CDATA[<p>在 Jetson AGX Orin&#x2F;Orin-X 平台的实际运行中，CUDA 与 DLA 原本各司其职，却因并发而偶发集体“假死”——所有线程在 <code>cudaStreamSynchronize()</code> 阻塞中寸步难行。面对这一似乎无解的死锁暗礁，我从故障复现场景入手，定位到混合推理流中的同步点；随后排除了显存越界、流类型、DLA 降频等常见因素；最终灵感来源于“反向隔离”思路——为 GPU 与 DLA 各自配置独立的 GPU Context，并通过显式的异步拷贝与同步保证数据无缝切换。<br>本文将以最直接的技术视角——从现象描述、排查方法到“多 Context 安全气囊”实现细节——全流程解读如何在 7×24h 压测中彻底消除死锁，实现并行推理的性能与可靠性双赢。</p><hr><h2 id="一、问题现象"><a href="#一、问题现象" class="headerlink" title="一、问题现象"></a>一、问题现象</h2><h3 id="1-偶发死锁"><a href="#1-偶发死锁" class="headerlink" title="1. 偶发死锁"></a>1. 偶发死锁</h3><ul><li><p><strong>症状描述</strong>：单进程内，所有线程在调用 GPU 时集体“假死”——<code>cudaStreamSynchronize()</code> 一直阻塞，<code>tegrastats</code> 显示 GPU 占用率维持在某个固定值，不再波动。  </p></li><li><p><strong>复现难度</strong>：  </p><ul><li>极不稳定：有时运气极差，一周都无法触发；有时运气极好，仅需两小时即复现。  </li><li>DLA 与 CUDA 混合推理时尤甚：DLA 推理线程先“提前几毫秒”挂死，其他线程随后也步入停滞。</li></ul><p><img src="/images/DLA_tegrastats.jpeg" alt="tegrastats 显示GPU卡死"></p><div style="display: flex; gap: 20px; margin: 1.5rem 0;">  <img src="/images/DLA_thread1.jpeg"       style="width: 100%; height: auto; border-radius: 4px;">  <img src="/images/DLA_thread2.jpeg"       style="width: 100%; height: auto; border-radius: 4px;"></div><figcaption style="text-align: center; color: #555; font-size: 0.9em; line-height: 1.4;">  GPU推理线程 与 DLA推理线程 同时卡死</figcaption></li></ul><h3 id="2-环境特征"><a href="#2-环境特征" class="headerlink" title="2. 环境特征"></a>2. 环境特征</h3><ul><li><strong>硬件平台</strong>：Jetson AGX Orin 系列（最初使用 AGX Orin，后转至 Orin-X）。  </li><li><strong>推理流程</strong>：  <ol><li>全部模型均通过 TensorRT 的 <code>enqueueV3()</code> + CUDA Stream 异步执行；  </li><li>前处理同样基于 CUDA Stream；  </li><li>部分子图运行于 DLA，非支持算子回落至 Tensor&#x2F;Tensor Core。</li></ol></li></ul><hr><h2 id="二、初步排查"><a href="#二、初步排查" class="headerlink" title="二、初步排查"></a>二、初步排查</h2><h3 id="1-定位死锁环节"><a href="#1-定位死锁环节" class="headerlink" title="1. 定位死锁环节"></a>1. 定位死锁环节</h3><p>通过在各关键步骤间插入 <code>cudaStreamSynchronize()</code>，逐步缩小排查范围：</p><ol><li><strong>前处理</strong> —— 无异常；  </li><li><strong>TensorRT <code>enqueueV3()</code></strong> —— 成功返回；  </li><li><strong>自定义 Plugin</strong>（HWCBGR → CHWRGB）—— 顺利完成；  </li><li><strong>同步点</strong> —— 在 Plugin 之后的第一次 <code>cudaStreamSynchronize()</code> 挂死。</li></ol><h3 id="2-排除思路"><a href="#2-排除思路" class="headerlink" title="2. 排除思路"></a>2. 排除思路</h3><ul><li><p><strong>显存越界</strong>？  </p><ul><li>其中一个模型 A 曾在 2023 年出现过显存越界，但通常显存越界会触发 sticky error 并抛出 <code>illegal memory access</code> 而非死锁。</li></ul></li><li><p><strong>Blocking vs. Non-blocking Stream</strong>？  </p><ul><li>初步猜测 DLA 可能依赖于 blocking stream，而我使用的是 non-blocking 版；最终未能证实。</li></ul></li><li><p><strong>DLA 频率降频</strong>？  </p><ul><li>若 DLA 调用低于 2 Hz，会自动释放并重申请资源，导致延迟飙升。  </li><li>提升调用频率后虽解决了降频，但死锁依旧。</li></ul><p><img src="/images/DLA_hz.jpg" alt="DLA降频导致延迟增加"></p></li></ul><hr><h2 id="三、环境变更与新线索"><a href="#三、环境变更与新线索" class="headerlink" title="三、环境变更与新线索"></a>三、环境变更与新线索</h2><h3 id="1-从-AGX-Orin-到-Orin-X"><a href="#1-从-AGX-Orin-到-Orin-X" class="headerlink" title="1. 从 AGX Orin 到 Orin-X"></a>1. 从 AGX Orin 到 Orin-X</h3><ul><li>2023 年 3 月，怀疑 AGX Orin DLA 变频机制有问题，遂放弃尝试；  </li><li>2024 年 3 月，转战 Orin-X，DLA 频率稳定——但在并行运行其他 GPU 模型 A 时，依旧触发死锁。</li></ul><h3 id="2-关键“怀疑模型-A”"><a href="#2-关键“怀疑模型-A”" class="headerlink" title="2. 关键“怀疑模型 A”"></a>2. 关键“怀疑模型 A”</h3><ul><li>模型 A 曾导致显存越界，虽然在最小化测试环境下并未复现死锁，但在系统中<strong>只要与 DLA 并行</strong>，死锁概率骤增，然而其余模型并不会导致该问题。</li><li>单独跑 DLA、单独跑模型 A 均正常，仅并行时“相逢即死锁”。</li></ul><hr><h2 id="四、“安全气囊”诞生：多-Context-隔离策略"><a href="#四、“安全气囊”诞生：多-Context-隔离策略" class="headerlink" title="四、“安全气囊”诞生：多 Context 隔离策略"></a>四、“安全气囊”诞生：多 Context 隔离策略</h2><h3 id="1-灵感来源"><a href="#1-灵感来源" class="headerlink" title="1. 灵感来源"></a>1. 灵感来源</h3><p>面试一位候选人提到的 GPU 多进程中间件：  </p><blockquote><p>“通过截获多进程的 GPU 请求，在后端融合 context，实现时分复用，从而提升GPU利用率。”  </p></blockquote><p>虽然该方案与我单进程场景相悖，却激发了一个思路——<strong>反其道而行之</strong>：既然单 context 并行 DLA + GPU 会死锁，何不为它们分别提供<strong>独立 context</strong>？</p><h3 id="2-实施要点"><a href="#2-实施要点" class="headerlink" title="2. 实施要点"></a>2. 实施要点</h3><ol><li><strong>上下文划分</strong>  <ul><li>Orin 系列拥有 2 个 DLA 引擎 + 1 个 GPU，引擎间资源隔离天然独立；  </li><li>为 DLA 和 GPU 各自创建独立 TensorRT Context。</li></ul></li><li><strong>内存与数据同步</strong>  <ul><li>注意：在 CUDA Context 之外，显存、Host 映射等资源也无法跨 context 共享；  </li><li>切换 Context 时，显式 <code>cudaMemcpyAsync()</code> 与 <code>cudaStreamSynchronize()</code> 确保数据同步与完整性。</li></ul></li><li><strong>并行执行</strong>  <ul><li>DLA 上跑模型 H；  </li><li>GPU 上跑模型 A、B、C、D、E；</li><li>彼此互不干扰，独立抢占各自算力。</li></ul></li></ol><hr><h2 id="五、最终验证"><a href="#五、最终验证" class="headerlink" title="五、最终验证"></a>五、最终验证</h2><ul><li><strong>7×24 h 连续压测</strong>：自 2024 年 4 月以来，未再出现任何死锁。  </li><li><strong>蝴蝶效应终成蝶</strong>：原本令人头痛的 DLA 隐晦死锁，反而催生出一套“安全气囊”式的多 context 并行机制——既排除了模型 A 的潜在越界风险，又保障了 DLA 与 GPU 资源的稳健协同。</li></ul><hr><h2 id="六、结语"><a href="#六、结语" class="headerlink" title="六、结语"></a>六、结语</h2><p>当我们在深度学习推理的微观世界里拨开层层迷雾，总会发现，看似偶然的卡死背后，往往是资源管理与并发调度的微妙博弈。通过多 Context 隔离，为 DLA 与 GPU 架起了一道“安全栅栏”，既兼顾了性能，又守护了系统的可用性——这，或许就是自动驾驶推理优化的下一块基石。</p><hr><p><em>—— 致正在与 DLA、TensorRT、CUDA “暗礁”博弈的你</em>  </p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在 Jetson AGX Orin&amp;#x2F;Orin-X 平台的实际运行中，CUDA 与 DLA 原本各司其职，却因并发而偶发集体“假死”——所有线程在 &lt;code&gt;cudaStreamSynchronize()&lt;/code&gt; 阻塞中寸步难行。面对这一似乎无解的死锁暗礁，</summary>
      
    
    
    
    <category term="blog" scheme="https://silvesterhsu.github.io/categories/blog/"/>
    
    
    <category term="Orin" scheme="https://silvesterhsu.github.io/tags/Orin/"/>
    
    <category term="GPU" scheme="https://silvesterhsu.github.io/tags/GPU/"/>
    
  </entry>
  
  <entry>
    <title>轻量化部署：Xavier设备低成本运行DeepSeek+QWQ大模型</title>
    <link href="https://silvesterhsu.github.io/2025/03/08/%E8%BD%BB%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2%EF%BC%9AXavier%E8%AE%BE%E5%A4%87%E4%BD%8E%E6%88%90%E6%9C%AC%E8%BF%90%E8%A1%8CDeepSeek-QWQ%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    <id>https://silvesterhsu.github.io/2025/03/08/%E8%BD%BB%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2%EF%BC%9AXavier%E8%AE%BE%E5%A4%87%E4%BD%8E%E6%88%90%E6%9C%AC%E8%BF%90%E8%A1%8CDeepSeek-QWQ%E5%A4%A7%E6%A8%A1%E5%9E%8B/</id>
    <published>2025-03-08T17:00:36.000Z</published>
    <updated>2025-05-16T06:30:46.786Z</updated>
    
    <content type="html"><![CDATA[<p>在边缘设备上部署大语言模型一直被认为是算力密集型的’禁区’，但当我用一块Jetson Xavier AGX成功运行了DeepSeek-R1 14B和QWQ-32B模型时，推理速度稳定在 8 tokens&#x2F;s (DeepSeek-R1 14B) &#x2F; 4 tokens&#x2F;s (QWQ-32B) 以上！本文将分享从环境配置到量化优化的完整方案，证明边缘设备同样可以成为轻量化AI的舞台。</p><hr><h2 id="一、硬件配置与挑战分析："><a href="#一、硬件配置与挑战分析：" class="headerlink" title="一、硬件配置与挑战分析："></a>一、硬件配置与挑战分析：</h2><ul><li>设备选型：NVIDIA Jetson Xavier AGX</li><li>核心参数：512核Volta GPU + 8核Carmel CPU &#x2F; 32GB RAM &#x2F; 32GB eMMC</li><li>系统环境：JetPack 5.1.3 (Ubuntu 20.04) &#x2F; CUDA 11.4 &#x2F; TensorRT 8.5.2</li><li>功耗模式：默认10W切换至MAXN模式</li></ul><h3 id="为什么选择DeepSeek-QWQ？"><a href="#为什么选择DeepSeek-QWQ？" class="headerlink" title="为什么选择DeepSeek &amp; QWQ？"></a>为什么选择DeepSeek &amp; QWQ？</h3><table><thead><tr><th align="left">维度</th><th align="left">QWQ-32B</th><th align="left">DeepSeek-R1-14B</th></tr></thead><tbody><tr><td align="left"><strong>参数量</strong></td><td align="left">32B</td><td align="left">14B</td></tr><tr><td align="left"><strong>架构类型</strong></td><td align="left">Decoder-Only（密集型）</td><td align="left">MoE（混合专家，16选2）</td></tr><tr><td align="left"><strong>上下文窗口</strong></td><td align="left">训练支持32K</td><td align="left">原生支持16K</td></tr><tr><td align="left"><strong>Xavier显存占用</strong></td><td align="left">20~22GB</td><td align="left">6~8GB</td></tr><tr><td align="left"><strong>推理速度</strong></td><td align="left">4.3 tokens&#x2F;s</td><td align="left">8.7 tokens&#x2F;s</td></tr><tr><td align="left"><strong>优势</strong></td><td align="left">长文本理解能力突出<br />工业领域知识深度优化<br />动态稀疏激活降低计算负载</td><td align="left">代码生成能力领先<br />多轮对话响应快<br />MoE架构灵活适配多任务</td></tr></tbody></table><h3 id="部署三大难关"><a href="#部署三大难关" class="headerlink" title="部署三大难关"></a>部署三大难关</h3><ol><li><strong>显存墙</strong>：FP16模型加载即需14GB+显存 → <strong>Jetson统一内存</strong></li><li><strong>架构差异</strong>：ARM平台与x86服务器的库兼容性问题 → <strong>Docker容器化</strong></li><li><strong>计算瓶颈</strong>：Token生成速度&lt;5tokens&#x2F;s时用户体验差 → <strong>流水线优化</strong></li></ol><h2 id="二、技术实现全流程拆解"><a href="#二、技术实现全流程拆解" class="headerlink" title="二、技术实现全流程拆解"></a>二、技术实现全流程拆解</h2><h3 id="阶段一：构建ARM适配的基础环境"><a href="#阶段一：构建ARM适配的基础环境" class="headerlink" title="阶段一：构建ARM适配的基础环境"></a>阶段一：构建ARM适配的基础环境</h3><p><strong>核心挑战</strong>：NVIDIA Jetson Xavier的ARM架构与常规x86服务器存在三大差异：</p><ol><li><strong>指令集差异</strong>：ARMv8.2与x86_64的SIMD指令集不兼容（如NEON vs AVX512）</li><li><strong>内存管理差异</strong>：Jetson采用CPU-GPU统一内存架构（UMA），与传统PCIe分体式显存管理模式冲突</li><li><strong>软件生态差异</strong>：PyTorch等框架的ARM预编译包缺失关键算子支持</li></ol><p><strong>技术路线</strong>：<br>采用「容器化隔离+定制化编译」双轨策略：</p><ol><li><strong>硬件抽象层</strong>：通过 <a href="https://github.com/dusty-nv/jetson-containers">jetson-containers</a> 项目预构建CUDA、cuDNN等核心组件的ARM64版本容器镜像</li><li><strong>依赖解耦</strong>：针对JetPack 5.1.3的特定版本（CUDA 11.4 + TensorRT 8.5.2），使用镜像解决动态库符号链接问题</li></ol><p><strong>关键实现</strong>：</p><ol><li><strong>更新python版本</strong>：jetson-containers代码依赖Python 3.9（<code>functools.cache</code>），需要将原有python3.8更新为python3.9版本</li></ol>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> <span class="built_in">rm</span> /usr/bin/python3 &amp;&amp; \</span><br><span class="line"><span class="built_in">sudo</span> <span class="built_in">ln</span> -s /usr/bin/python3.9 /usr/bin/python3</span><br></pre></td></tr></table></figure><ol start="2"><li><strong>安装jetson-containers</strong>：</li></ol>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/dusty-nv/jetson-containers</span><br><span class="line">bash jetson-containers/install.sh</span><br></pre></td></tr></table></figure><ol start="3"><li><strong>设置Docker Runtime</strong>：<br>  修改<code>/etc/docker/daemon.json</code>配置，并指定<code>default-runtime</code></li></ol>  <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;runtimes&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;nvidia&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;nvidia-container-runtime&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;runtimeArgs&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">&quot;default-runtime&quot;</span><span class="punctuation">:</span> <span class="string">&quot;nvidia&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>  重启docker服务<br>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> systemctl restart docker</span><br></pre></td></tr></table></figure><br>  验证配置更新<br>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> docker info | grep <span class="string">&#x27;Default Runtime&#x27;</span></span><br></pre></td></tr></table></figure></p><blockquote><p>显示docker没有root权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> usermod -aG docker <span class="variable">$USER</span></span><br></pre></td></tr></table></figure></blockquote><h3 id="阶段二：构建Ollama基础镜像"><a href="#阶段二：构建Ollama基础镜像" class="headerlink" title="阶段二：构建Ollama基础镜像"></a>阶段二：构建Ollama基础镜像</h3><ol><li><strong>修改Dockerfile</strong><br>  修改<code>jetson-containers/packages/llm/ollama/Dockerfile</code>为以下内容</li></ol>  <figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#---</span></span><br><span class="line"><span class="comment"># name: ollama</span></span><br><span class="line"><span class="comment"># group: llm</span></span><br><span class="line"><span class="comment"># config: config.py</span></span><br><span class="line"><span class="comment"># depends: cuda</span></span><br><span class="line"><span class="comment"># requires: &#x27;&gt;=34.1.0&#x27;</span></span><br><span class="line"><span class="comment"># docs: docs.md</span></span><br><span class="line"><span class="comment">#---</span></span><br><span class="line"><span class="keyword">ARG</span> BASE_IMAGE \</span><br><span class="line">    CMAKE_CUDA_ARCHITECTURES \</span><br><span class="line">    CUDA_VERSION_MAJOR \</span><br><span class="line">    JETPACK_VERSION \</span><br><span class="line">    OLLAMA_REPO \</span><br><span class="line">    OLLAMA_BRANCH \</span><br><span class="line">    GOLANG_VERSION \</span><br><span class="line">    CMAKE_VERSION</span><br><span class="line"></span><br><span class="line"><span class="comment"># build the runtime container</span></span><br><span class="line"><span class="keyword">FROM</span> $&#123;BASE_IMAGE&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">ARG</span> JETPACK_VERSION \</span><br><span class="line">    CUDA_VERSION_MAJOR</span><br><span class="line"></span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">11434</span></span><br><span class="line"><span class="keyword">ENV</span> OLLAMA_HOST=<span class="number">0.0</span>.<span class="number">0.0</span> \</span><br><span class="line">    OLLAMA_MODELS=/data/models/ollama/models \</span><br><span class="line">    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \</span><br><span class="line">    LD_LIBRARY_PATH=/usr/local/cuda/lib:/usr/local/cuda/lib64:/usr/local/cuda/include:$&#123;LD_LIBRARY_PATH&#125; \</span><br><span class="line">    JETSON_JETPACK=$&#123;JETPACK_VERSION&#125; \</span><br><span class="line">    CUDA_VERSION_MAJOR=$&#123;CUDA_VERSION_MAJOR&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> apt-get update &amp;&amp; \</span></span><br><span class="line"><span class="language-bash">    apt install -y curl  &amp;&amp; \</span></span><br><span class="line"><span class="language-bash">    <span class="built_in">rm</span> -rf /var/lib/apt/lists/* &amp;&amp; \</span></span><br><span class="line"><span class="language-bash">    apt-get clean</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> nv_tegra_release /etc/nv_tegra_release</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#ADD https://api.github.com/repos/ollama/ollama/branches/main /tmp/ollama_version.json</span></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> curl -H <span class="string">&quot;Authorization: token <span class="variable">$&#123;GITHUB_TOKEN&#125;</span>&quot;</span> \</span></span><br><span class="line"><span class="language-bash">    -o /tmp/ollama_version.json \</span></span><br><span class="line"><span class="language-bash">    https://api.github.com/repos/ollama/ollama/branches/main</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> curl -fsSL https://ollama.com/install.sh | sh</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> <span class="built_in">rm</span> /usr/bin/python || <span class="literal">true</span> &amp;&amp; \</span></span><br><span class="line"><span class="language-bash">    <span class="built_in">ln</span> -s /usr/bin/python3 /usr/bin/python</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> start_ollama /</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="language-bash"> /start_ollama &amp;&amp; /bin/bash</span></span><br></pre></td></tr></table></figure><ol start="2"><li><strong>构建镜像</strong></li></ol>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jetson-containers build ollama</span><br></pre></td></tr></table></figure><ol start="3"><li><strong>验证构建镜像</strong></li></ol>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker images | grep ollama</span><br></pre></td></tr></table></figure><h3 id="阶段三：模型下载-推理"><a href="#阶段三：模型下载-推理" class="headerlink" title="阶段三：模型下载 &amp; 推理"></a>阶段三：模型下载 &amp; 推理</h3><ol><li><strong>进入Container环境</strong></li></ol>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jetson-containers run --name ollama $(autotag ollama)</span><br></pre></td></tr></table></figure><ol start="2"><li><strong>模型下载</strong></li></ol>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ollama pull qwq:latest</span><br><span class="line">ollama pull deepseek-r1:14b</span><br></pre></td></tr></table></figure><ol start="3"><li><strong>模型推理验证</strong></li></ol>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama run qwq:latest</span><br></pre></td></tr></table></figure><p>  查看GPU使用率<br>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jtop</span><br></pre></td></tr></table></figure></p><blockquote><p><a href="https://github.com/rbonghi/jetson_stats">jtop</a>安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> pip3 install -U jetson-stats &amp;&amp; \</span><br><span class="line"><span class="built_in">sudo</span> systemctl restart jtop.service</span><br></pre></td></tr></table></figure></blockquote><h3 id="阶段四（可选）：K3S部署与服务搭建"><a href="#阶段四（可选）：K3S部署与服务搭建" class="headerlink" title="阶段四（可选）：K3S部署与服务搭建"></a>阶段四（可选）：K3S部署与服务搭建</h3><p>待补充</p><h2 id="三、性能实测：数据不说谎"><a href="#三、性能实测：数据不说谎" class="headerlink" title="三、性能实测：数据不说谎"></a>三、性能实测：数据不说谎</h2><h3 id="模型对比实验"><a href="#模型对比实验" class="headerlink" title="模型对比实验"></a>模型对比实验</h3><table><thead><tr><th align="left">模型</th><th align="left">显存</th><th align="left">待机功耗</th><th align="left">运行功耗</th><th>prompt eval rate</th><th>eval rate</th></tr></thead><tbody><tr><td align="left">deepseek-r1:14b</td><td align="left">10.5G</td><td align="left">5.1w</td><td align="left">43.7 w</td><td>144.21 tokens&#x2F;s</td><td>8.11 tokens&#x2F;s</td></tr><tr><td align="left">QwQ:32b</td><td align="left">21.1G</td><td align="left">5.4 w</td><td align="left">47.6 w</td><td>6.04 tokens&#x2F;s</td><td>3.93 tokens&#x2F;s</td></tr></tbody></table><h4 id="deepseek-r1-14b"><a href="#deepseek-r1-14b" class="headerlink" title="deepseek-r1:14b"></a>deepseek-r1:14b</h4><div style="display: flex; gap: 20px; margin: 1.5rem 0;">  <img src="/images/xavier_deepseek.jpg"        style="width: 100%; height: auto; border-radius: 8px;">  <img src="/images/xavier_deepseek_gpu.jpg"        style="width: 100%; height: auto; border-radius: 8px;"></div><br><h4 id="QwQ-32b"><a href="#QwQ-32b" class="headerlink" title="QwQ:32b"></a>QwQ:32b</h4><div style="display: flex; gap: 20px; margin: 1.5rem 0;">  <img src="/images/xavier_qwq.jpg"        style="width: 100%; height: auto; border-radius: 8px;">  <img src="/images/xavier_qwq_gpu.jpg"        style="width: 100%; height: auto; border-radius: 8px;"></div><br><h3 id="成本效益对比"><a href="#成本效益对比" class="headerlink" title="成本效益对比"></a>成本效益对比</h3><table><thead><tr><th align="left">方案</th><th align="left">硬件成本</th><th align="left">每月费用</th><th align="left">延迟</th><th align="left">数据出境风险</th></tr></thead><tbody><tr><td align="left">云端API</td><td align="left">¥ 0</td><td align="left">¥1600</td><td align="left">200ms</td><td align="left">高</td></tr><tr><td align="left"><strong>Xavier本地</strong></td><td align="left">¥ 3000</td><td align="left">¥18</td><td align="left">20ms</td><td align="left">零</td></tr></tbody></table><h2 id="应用场景：这不是玩具，是生产力"><a href="#应用场景：这不是玩具，是生产力" class="headerlink" title="应用场景：这不是玩具，是生产力"></a>应用场景：这不是玩具，是生产力</h2><h3 id="案例1：openweb-ui知识库本地问答"><a href="#案例1：openweb-ui知识库本地问答" class="headerlink" title="案例1：openweb-ui知识库本地问答"></a>案例1：openweb-ui知识库本地问答</h3><p>待补充</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在边缘设备上部署大语言模型一直被认为是算力密集型的’禁区’，但当我用一块Jetson Xavier AGX成功运行了DeepSeek-R1 14B和QWQ-32B模型时，推理速度稳定在 8 tokens&amp;#x2F;s (DeepSeek-R1 14B) &amp;#x2F; 4 t</summary>
      
    
    
    
    <category term="blog" scheme="https://silvesterhsu.github.io/categories/blog/"/>
    
    
    <category term="GPU" scheme="https://silvesterhsu.github.io/tags/GPU/"/>
    
    <category term="Xavier" scheme="https://silvesterhsu.github.io/tags/Xavier/"/>
    
  </entry>
  
  <entry>
    <title>从源码构建 Jetson-Perf：一步步指南</title>
    <link href="https://silvesterhsu.github.io/2025/02/17/%E4%BB%8E%E6%BA%90%E7%A0%81%E6%9E%84%E5%BB%BAJetson-Perf/"/>
    <id>https://silvesterhsu.github.io/2025/02/17/%E4%BB%8E%E6%BA%90%E7%A0%81%E6%9E%84%E5%BB%BAJetson-Perf/</id>
    <published>2025-02-17T14:22:51.000Z</published>
    <updated>2025-05-16T06:30:46.786Z</updated>
    
    <content type="html"><![CDATA[<p>在 NVIDIA Jetson 平台上进行性能分析时，perf 工具是不可或缺的利器。然而，由于硬件架构和内核版本的特殊性，直接使用预编译版本可能无法完全满足需求。本文将从源码开始，详细介绍如何在 Jetson Orin 和 X86 平台上构建 perf 工具，并解决编译过程中可能遇到的常见问题。</p><hr><h2 id="一、准备工作：源码下载"><a href="#一、准备工作：源码下载" class="headerlink" title="一、准备工作：源码下载"></a>一、准备工作：源码下载</h2><h3 id="Jetson-Orin-平台"><a href="#Jetson-Orin-平台" class="headerlink" title="Jetson Orin 平台"></a>Jetson Orin 平台</h3><ol><li><strong>下载 L4T 驱动包</strong><br>访问 <a href="https://developer.nvidia.com/embedded/downloads/">NVIDIA 开发者下载中心</a>，搜索 <strong>Jetson Linux Driver Package (L4T)</strong>，选择与设备匹配的版本（如 R35.x 系列）。<br>[注：需根据 Jetson 型号选择对应的版本，参考下图中的版本号。]<br><img src="/images/Jetson_L4T.png"></li><li><strong>获取 BSP 源码</strong><br>下载完成后，找到 <code>public_sources.tbz2</code> 文件，这是构建内核和工具链所需的基础源码包。<br><img src="/images/BSP.png"></li></ol><br><h3 id="Orin-Drive-平台"><a href="#Orin-Drive-平台" class="headerlink" title="Orin Drive 平台"></a>Orin Drive 平台</h3><ol><li><p><strong>查看当前系统的核版本</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">uname</span> -r</span><br></pre></td></tr></table></figure></li><li><p><strong>下载内核源码</strong><br>访问 <a href="https://mirrors.edge.kernel.org/pub/linux/kernel/">Linux 内核镜像站</a>，根据当前系统内核版本下载对应的源码包。<br>例如：若内核版本为 <code>5.15.116-rt-tegra</code>，则下载 <code>linux-5.15.116.tar.gz</code>。</p></li></ol><br><h3 id="X86-平台"><a href="#X86-平台" class="headerlink" title="X86 平台"></a>X86 平台</h3><ol><li><p><strong>查看当前系统的核版本</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">uname</span> -r</span><br></pre></td></tr></table></figure></li><li><p><strong>下载内核源码</strong><br>访问 <a href="https://mirrors.edge.kernel.org/pub/linux/kernel/">Linux 内核镜像站</a>，根据当前系统内核版本下载对应的源码包。<br>例如：若内核版本为 <code>5.10.0</code>，则下载 <code>linux-5.10.tar.gz</code>。</p></li></ol><br><h2 id="二、源码编译步骤"><a href="#二、源码编译步骤" class="headerlink" title="二、源码编译步骤"></a>二、源码编译步骤</h2><h3 id="编译环境准备"><a href="#编译环境准备" class="headerlink" title="编译环境准备"></a>编译环境准备</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> apt install make gcc flex bison pkg-config -y</span><br></pre></td></tr></table></figure><br><h3 id="Jetson-Orin-编译流程"><a href="#Jetson-Orin-编译流程" class="headerlink" title="Jetson Orin 编译流程"></a>Jetson Orin 编译流程</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tar -xjvf public_sources.tbz2 &amp;&amp; \</span><br><span class="line"><span class="built_in">cd</span> Linux_for_Tegra/source/public &amp;&amp; \</span><br><span class="line"></span><br><span class="line">tar -xjvf kernel_src.tbz2 &amp;&amp; \</span><br><span class="line"><span class="built_in">cd</span> kernel/kernel-5.10/tools/perf &amp;&amp; \</span><br><span class="line"></span><br><span class="line">make -j$(<span class="built_in">nproc</span>) &amp;&amp; \</span><br><span class="line"><span class="built_in">sudo</span> make install &amp;&amp; \</span><br><span class="line"></span><br><span class="line">./perf --version</span><br></pre></td></tr></table></figure><br><h3 id="Orin-Drive-编译流程"><a href="#Orin-Drive-编译流程" class="headerlink" title="Orin Drive 编译流程"></a>Orin Drive 编译流程</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tar zxvf linux-5.15.116.tar.gz &amp;&amp; \</span><br><span class="line"><span class="built_in">cd</span> linux-5.15.116/tools/perf &amp;&amp; \</span><br><span class="line"></span><br><span class="line">make -j$(<span class="built_in">nproc</span>) &amp;&amp; \</span><br><span class="line"><span class="built_in">sudo</span> make install &amp;&amp; \</span><br><span class="line"></span><br><span class="line">./perf --version</span><br><span class="line"></span><br></pre></td></tr></table></figure><br><h3 id="X86-平台编译流程"><a href="#X86-平台编译流程" class="headerlink" title="X86 平台编译流程"></a>X86 平台编译流程</h3><ol><li><p><strong>安装依赖项</strong><br>X86 平台需安装以下开发库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> apt-get install build-essential flex bison python3 python3-dev \</span><br><span class="line">  libelf-dev libnewt-dev libdw-dev libaudit-dev libiberty-dev \</span><br><span class="line">  libunwind-dev libcap-dev libzstd-dev libnuma-dev libssl-dev \</span><br><span class="line">  binutils-dev gcc-multilib liblzma-dev</span><br></pre></td></tr></table></figure></li><li><p><strong>编译 perf</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tar zxvf linux-5.10.tar.gz &amp;&amp; \</span><br><span class="line"><span class="built_in">cd</span> linux-5.10/tools/perf &amp;&amp; \</span><br><span class="line"></span><br><span class="line">make -j$(<span class="built_in">nproc</span>) &amp;&amp; \</span><br><span class="line"><span class="built_in">sudo</span> make install &amp;&amp; \</span><br><span class="line"></span><br><span class="line">./perf --version</span><br></pre></td></tr></table></figure></li></ol><br><h2 id="三、开启内核调试支持"><a href="#三、开启内核调试支持" class="headerlink" title="三、开启内核调试支持"></a>三、开启内核调试支持</h2><p>默认情况下，Linux 内核会限制 <code>perf</code> 的权限。通过以下命令解除限制：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> sysctl kernel.perf_event_paranoid=-1</span><br></pre></td></tr></table></figure><p>将此设置永久生效，可将其写入 <code>/etc/sysctl.conf</code>。</p><br><h2 id="四、通过-perf-生成火焰图：直观分析性能瓶颈"><a href="#四、通过-perf-生成火焰图：直观分析性能瓶颈" class="headerlink" title="四、通过 perf 生成火焰图：直观分析性能瓶颈"></a>四、通过 <code>perf</code> 生成火焰图：直观分析性能瓶颈</h2><p>火焰图（Flame Graph）是性能分析的利器，它能以可视化方式展示程序的函数调用栈和资源占用情况。结合 <code>perf</code> 工具，我们可以快速生成火焰图，精准定位性能瓶颈。以下是详细操作步骤：</p><h3 id="1-安装火焰图生成工具"><a href="#1-安装火焰图生成工具" class="headerlink" title="1. 安装火焰图生成工具"></a>1. 安装火焰图生成工具</h3><p>首先需要克隆 Brendan Gregg 的 <strong>FlameGraph</strong> 工具库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/brendangregg/FlameGraph.git</span><br><span class="line"><span class="built_in">cd</span> FlameGraph</span><br></pre></td></tr></table></figure><p>将此工具的路径加入环境变量，或直接通过绝对路径调用脚本（如 <code>./FlameGraph/stackcollapse-perf.pl</code>）。</p><h3 id="2-使用-perf-采集性能数据"><a href="#2-使用-perf-采集性能数据" class="headerlink" title="2. 使用 perf 采集性能数据"></a>2. 使用 <code>perf</code> 采集性能数据</h3><p>运行目标程序，并通过 <code>perf record</code> 记录性能事件（如 CPU 周期、缓存失效等）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 监控 CPU 使用情况（默认）</span></span><br><span class="line"><span class="built_in">sudo</span> perf record -F 99 -a -g -- <span class="built_in">sleep</span> 60</span><br><span class="line"><span class="comment"># 或监控特定进程</span></span><br><span class="line"><span class="built_in">sudo</span> perf record -F 99 -p &lt;PID&gt; -g -- <span class="built_in">sleep</span> 30</span><br><span class="line"><span class="comment"># 或监控具体程序</span></span><br><span class="line"><span class="built_in">sudo</span> perf record -F 99 -g ./&lt;MY_PROGRAM&gt;</span><br></pre></td></tr></table></figure><ul><li><strong>参数说明</strong>：<ul><li><code>-F 99</code>：采样频率为 99 Hz（避免与某些内核频率冲突）。</li><li><code>-a</code>：监控所有 CPU。</li><li><code>-g</code>：记录调用栈（生成火焰图必需）。</li><li><code>-- sleep 60</code>：持续采样 60 秒。</li></ul></li></ul><h3 id="3-生成火焰图"><a href="#3-生成火焰图" class="headerlink" title="3. 生成火焰图"></a>3. 生成火焰图</h3><p>采集完成后，按以下步骤处理数据：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成 perf.data 的解析文本</span></span><br><span class="line"><span class="built_in">sudo</span> perf script &gt; out.perf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 折叠调用栈（关键步骤）</span></span><br><span class="line">./FlameGraph/stackcollapse-perf.pl out.perf &gt; out.folded</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成 SVG 格式火焰图</span></span><br><span class="line">./FlameGraph/flamegraph.pl out.folded &gt; flamegraph.svg</span><br></pre></td></tr></table></figure><h3 id="4-查看与分析火焰图"><a href="#4-查看与分析火焰图" class="headerlink" title="4. 查看与分析火焰图"></a>4. 查看与分析火焰图</h3><p>用浏览器打开 <code>flamegraph.svg</code>，可以看到类似下图的交互式可视化结果：<br><img src="/images/perf.jpg"></p><p><strong>火焰图解读技巧</strong>：</p><ul><li><strong>横向宽度</strong>：表示函数或代码路径的资源（如 CPU 时间）占用比例。</li><li><strong>纵向层级</strong>：表示函数调用栈的深度，顶层是正在执行的函数，下层是调用者。</li><li><strong>点击缩放</strong>：支持点击任意区块展开查看细节。</li><li><strong>高亮搜索</strong>：按 <code>Ctrl + F</code> 搜索关键函数名。</li></ul><br><h2 id="五、常见问题解决（FAQ）"><a href="#五、常见问题解决（FAQ）" class="headerlink" title="五、常见问题解决（FAQ）"></a>五、常见问题解决（FAQ）</h2><h3 id="Q1：编译时提示某些-Feature-未启用"><a href="#Q1：编译时提示某些-Feature-未启用" class="headerlink" title="Q1：编译时提示某些 Feature 未启用"></a>Q1：编译时提示某些 Feature 未启用</h3><p><strong>问题现象</strong><br>编译过程中出现 <code>WARNING: ... missing xxx,某些功能将被禁用</code>。<br><img src="/images/perf_feature.png"></p><p><strong>解决方法</strong></p><ol><li><p>查看具体错误日志：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> ../build/feature/test-libunwind.make.output</span><br></pre></td></tr></table></figure><p><img src="/images/perf_error.jpg"></p></li><li><p>根据缺失的依赖安装对应开发库。<br>安装<code>elf</code>即可<code>sudo apt install -y libelf-dev</code>，其余Feature开启方法同理</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> apt install -y libelf-dev</span><br><span class="line"><span class="built_in">sudo</span> apt install libunwind-dev</span><br><span class="line"><span class="built_in">sudo</span> apt install -y libdw1</span><br><span class="line"><span class="built_in">sudo</span> apt install -y elfutils</span><br><span class="line"><span class="built_in">sudo</span> apt install -y libdw-dev</span><br><span class="line"><span class="built_in">sudo</span> apt install -y binutils-dev</span><br></pre></td></tr></table></figure></li></ol><h3 id="Q2：运行-perf-时权限不足"><a href="#Q2：运行-perf-时权限不足" class="headerlink" title="Q2：运行 perf 时权限不足"></a>Q2：运行 perf 时权限不足</h3><p>若未设置 <code>kernel.perf_event_paranoid</code>，需通过 <code>sudo</code> 运行 <code>perf</code>，或按第三步永久解除限制。</p><br><h2 id="六、总结"><a href="#六、总结" class="headerlink" title="六、总结"></a>六、总结</h2><p>通过从源码构建 <code>perf</code> 工具，可以确保其与当前内核版本的完全兼容性，并启用所有高级功能。如果在编译中遇到问题，请优先检查依赖项和错误日志，大多数问题均可通过安装缺失的库解决。</p><hr><p><strong>效果验证</strong><br>成功编译后，运行 <code>perf list</code> 可查看支持的性能监控事件。尝试使用 <code>perf stat</code> 或 <code>perf record</code> 分析程序性能。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在 NVIDIA Jetson 平台上进行性能分析时，perf 工具是不可或缺的利器。然而，由于硬件架构和内核版本的特殊性，直接使用预编译版本可能无法完全满足需求。本文将从源码开始，详细介绍如何在 Jetson Orin 和 X86 平台上构建 perf 工具，并解决编译过</summary>
      
    
    
    
    <category term="blog" scheme="https://silvesterhsu.github.io/categories/blog/"/>
    
    
    <category term="Orin" scheme="https://silvesterhsu.github.io/tags/Orin/"/>
    
    <category term="GPU" scheme="https://silvesterhsu.github.io/tags/GPU/"/>
    
    <category term="性能分析" scheme="https://silvesterhsu.github.io/tags/%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
</feed>
